#!/usr/bin/env python3
"""
PostgreSQL pgvector + Azure OpenAI ê¸°ë°˜ RAG ì±„íŒ… ì‹œìŠ¤í…œ
- ì„ë² ë”©: Ollama bge-m3 (1024ì°¨ì›)
- LLM: Azure GPT-4o-mini
"""

import asyncio
import json
import subprocess
import asyncpg
import httpx
from typing import AsyncGenerator
from openai import AzureOpenAI

# PostgreSQL ì„¤ì •
POSTGRES_HOST = "postgres"
POSTGRES_PORT = 5432
POSTGRES_USER = "n8n"
POSTGRES_PASSWORD = "n8n123"
POSTGRES_DATABASE = "n8n"

# Ollama ì„ë² ë”© ì„¤ì •
OLLAMA_HOST = "http://localhost:11434"
EMBEDDING_MODEL = "bge-m3"

# Azure OpenAI ì„¤ì •
import os
AZURE_API_VERSION = "2024-08-01-preview"
AZURE_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "https://etech-openai.openai.azure.com/")
AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY", "")
AZURE_MODEL = "gpt-4o-mini"

WORKSPACE = "default"

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸
azure_client = AzureOpenAI(
    api_version=AZURE_API_VERSION,
    azure_endpoint=AZURE_ENDPOINT,
    api_key=AZURE_API_KEY
)


def get_embedding_curl(text: str) -> list:
    """curlì„ ì‚¬ìš©í•˜ì—¬ Ollamaì—ì„œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°"""
    payload = json.dumps({"model": EMBEDDING_MODEL, "input": text[:8000]})

    result = subprocess.run(
        ["curl", "-s", "--max-time", "60",
         f"{OLLAMA_HOST}/api/embed",
         "-H", "Content-Type: application/json",
         "-d", payload],
        capture_output=True,
        text=True,
        timeout=120
    )

    if result.returncode == 0 and result.stdout:
        data = json.loads(result.stdout)
        embeddings = data.get("embeddings", [])
        if embeddings and len(embeddings) > 0:
            return embeddings[0]
    return None


async def search_relevant_context(conn, query_vector: list, top_k: int = 5) -> str:
    """ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    vector_str = '[' + ','.join(map(str, query_vector)) + ']'

    # ì—”í‹°í‹° ê²€ìƒ‰
    entities = await conn.fetch(f"""
        SELECT entity_name, content,
               content_vector <=> '{vector_str}'::vector AS distance
        FROM lightrag_vdb_entity
        WHERE workspace = $1
        ORDER BY content_vector <=> '{vector_str}'::vector
        LIMIT $2
    """, WORKSPACE, top_k)

    # ê´€ê³„ ê²€ìƒ‰
    relations = await conn.fetch(f"""
        SELECT source_id, target_id, content,
               content_vector <=> '{vector_str}'::vector AS distance
        FROM lightrag_vdb_relation
        WHERE workspace = $1
        ORDER BY content_vector <=> '{vector_str}'::vector
        LIMIT 3
    """, WORKSPACE)

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []

    context_parts.append("## ê´€ë ¨ ê°œë…/ì—”í‹°í‹°:")
    for ent in entities:
        if ent['distance'] < 0.6:  # ìœ ì‚¬ë„ ì„ê³„ê°’
            context_parts.append(f"- {ent['entity_name']}: {ent['content'][:300]}")

    context_parts.append("\n## ê´€ë ¨ ê´€ê³„:")
    for rel in relations:
        if rel['distance'] < 0.6:
            context_parts.append(f"- {rel['source_id']} â†’ {rel['target_id']}: {rel['content'][:200]}")

    return "\n".join(context_parts)


async def chat_stream(query: str, history: list = None) -> AsyncGenerator[str, None]:
    """ìŠ¤íŠ¸ë¦¬ë° RAG ì±„íŒ… (Azure OpenAI)"""
    if history is None:
        history = []

    # 1. ì„ë² ë”© ìƒì„± (Ollama bge-m3)
    query_vector = get_embedding_curl(query)
    if not query_vector:
        yield "ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        return

    # 2. PostgreSQL ì—°ê²° ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    conn = await asyncpg.connect(
        host=POSTGRES_HOST,
        port=POSTGRES_PORT,
        user=POSTGRES_USER,
        password=POSTGRES_PASSWORD,
        database=POSTGRES_DATABASE
    )

    try:
        context = await search_relevant_context(conn, query_vector)
    finally:
        await conn.close()

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ì´ë²„ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¼ë°˜ì ì¸ ë³´ì•ˆ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ë˜, ì¶”ì¸¡ì„ì„ ëª…ì‹œí•˜ì„¸ìš”.

### ì»¨í…ìŠ¤íŠ¸:
""" + context

    # 4. Azure OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    messages = [{"role": "system", "content": system_prompt}]

    # íˆìŠ¤í† ë¦¬ ì¶”ê°€
    for msg in history:
        messages.append(msg)

    messages.append({"role": "user", "content": query})

    try:
        stream = azure_client.chat.completions.create(
            model=AZURE_MODEL,
            messages=messages,
            temperature=0.7,
            stream=True
        )

        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    yield delta.content

    except Exception as e:
        yield f"\nAzure OpenAI ì˜¤ë¥˜: {e}"


async def chat_complete(query: str, history: list = None) -> str:
    """ë¹„ìŠ¤íŠ¸ë¦¬ë° RAG ì±„íŒ… (ì „ì²´ ì‘ë‹µ ë°˜í™˜)"""
    response_parts = []
    async for chunk in chat_stream(query, history):
        response_parts.append(chunk)
    return "".join(response_parts)


async def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ì‚¬ì´ë²„ë³´ì•ˆ RAG ì±„íŒ… ì‹œìŠ¤í…œ (PostgreSQL + Ollama)      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ëª…ë ¹ì–´: 'quit' ë˜ëŠ” 'exit'ë¡œ ì¢…ë£Œ, 'clear'ë¡œ ëŒ€í™” ì´ˆê¸°í™”
    """)

    history = []

    while True:
        try:
            query = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()

            if not query:
                continue

            if query.lower() in ['quit', 'exit', 'q']:
                print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if query.lower() == 'clear':
                history = []
                print("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                continue

            print("\nğŸ¤– ë‹µë³€: ", end="", flush=True)

            response_text = ""
            async for chunk in chat_stream(query, history):
                print(chunk, end="", flush=True)
                response_text += chunk

            print()  # ì¤„ë°”ê¿ˆ

            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            history.append({"role": "user", "content": query})
            history.append({"role": "assistant", "content": response_text})

            # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
            if len(history) > 10:
                history = history[-10:]

        except KeyboardInterrupt:
            print("\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys

    if len(sys.argv) > 1:
        # ì»¤ë§¨ë“œë¼ì¸ ì¸ìê°€ ìˆìœ¼ë©´ ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬
        query = " ".join(sys.argv[1:])
        print(f"ì§ˆë¬¸: {query}\n")
        print("ë‹µë³€: ", end="", flush=True)
        async for chunk in chat_stream(query):
            print(chunk, end="", flush=True)
        print()
    else:
        # ì¸ìê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
        await interactive_chat()


if __name__ == "__main__":
    asyncio.run(main())
